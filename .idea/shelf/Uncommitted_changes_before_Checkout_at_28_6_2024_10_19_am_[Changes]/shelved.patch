Index: Desko.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\r\nimport numpy as np\r\nimport math\r\nimport copy\r\nimport torch.distributions as dist\r\nimport torch.distributions.transforms as transforms\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.distributions as torchd\r\nfrom torch.utils.data import Dataset, DataLoader, random_split\r\nfrom matplotlib import pyplot as plt\r\nimport torch.nn.functional as F\r\n\r\nfrom three_tanks import three_tank_system\r\n\r\n#===========for matplotlib==============#\r\nimport os\r\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\r\n\r\n\r\n\r\n\r\n\r\nSCALE_DIAG_MIN_MAX = (-20, 2)\r\ndef weights_init(m):\r\n    classname = m.__class__.__name__\r\n    if classname.find('Conv2d') != -1:\r\n        nn.init.xavier_normal_(m.weight.data)\r\n        nn.init.constant_(m.bias.data, 0.0)\r\n    elif classname.find('Linear') != -1:\r\n        nn.init.xavier_normal_(m.weight)\r\n        nn.init.constant_(m.bias, 0.0)\r\n\r\nclass Koopman_Desko(object):\r\n    \"\"\"\r\n    feature:\r\n    -Encoder-decoder\r\n    -LSTM\r\n    -Deterministic\r\n\r\n    \"\"\"\r\n    def __init__(\r\n        self,\r\n        args,\r\n        **kwargs\r\n    ):\r\n        self.shift = None\r\n        self.scale = None\r\n        self.shift_u = None\r\n        self.scale_u = None\r\n\r\n        self.d = torch.nn.Parameter(torch.ones(1).to(args['device']), requires_grad=True) # data loss weight parameter\r\n        self.p2 = torch.nn.Parameter(torch.ones(1).to(args['device']),\r\n                                     requires_grad=True)  # physics loss weight parameter\r\n        self.p3 = torch.nn.Parameter(torch.ones(1).to(args['device']),\r\n                                     requires_grad=True)# physics loss weight parameter\r\n\r\n        # self.loss = 0\r\n        # self.d_loss = 0  # data loss\r\n        # self.p2_loss = 0\r\n        # self.p3_loss = 0  # physics loss\r\n\r\n        if args['ABCD'] == 1:\r\n            if args['extend_state']:\r\n                # 用这个\r\n                self._create_koopman_matrix_a1_extend(args)\r\n            else:\r\n                self._create_koopman_matrix_a1(args)\r\n        elif args['ABCD'] == 2:\r\n            if args['extend_state']:\r\n                self._create_koopman_matrix_a2_extend(args)\r\n            else:\r\n                self._create_koopman_matrix_a2(args)\r\n\r\n        self.net = MLP(args).to(args['device'])\r\n        self.net.apply(weights_init)\r\n\r\n        self.noisemlp = Noise_MLP(args).to(args['device'])\r\n\r\n        self.net_para = {}\r\n        self.noise_para = {}\r\n\r\n        self.loss_buff = 100000\r\n\r\n\r\n        # self.optimizer = optim.Adam([{'params': self.net.parameters(),'lr':args['lr']},{'params': [self.A_1,self.B_1,self.C_1,],'lr':args['lr']}])\r\n\r\n        self.optimizer1 = optim.Adam([{'params': self.net.parameters(), 'lr': args['lr1'], 'weight_decay':args['weight_decay']},\r\n                                      {'params': self.noisemlp.parameters(), 'lr': args['lr1'], 'weight_decay': args['weight_decay']}])\r\n        self.optimizer1_sch = torch.optim.lr_scheduler.StepLR(self.optimizer1, step_size=args['optimize_step'], gamma=args['gamma'])\r\n\r\n        if args['ABCD'] != 2:\r\n            self.optimizer2 = optim.Adam([{'params': [self.A_1,self.B_1,self.C_1,],'lr':args['lr2'],'weight_decay':args['weight_decay']}])\r\n            self.optimizer2_sch = torch.optim.lr_scheduler.StepLR(self.optimizer2, step_size=args['optimize_step'], gamma=args['gamma'])\r\n\r\n        self.optimizer3 = optim.Adam(\r\n            [{'params': self.d, 'lr': args['lr3'], 'weight_decay': args['weight_decay']},\r\n             {'params': self.p2, 'lr': args['lr3'], 'weight_decay': args['weight_decay']},\r\n             {'params': self.p3, 'lr': args['lr3'], 'weight_decay': args['weight_decay']}])\r\n        self.optimizer3_sch = torch.optim.lr_scheduler.StepLR(self.optimizer3, step_size=args['optimize_step'],\r\n                                                              gamma=args['gamma'])\r\n\r\n        self.MODEL_SAVE = args['MODEL_SAVE']\r\n        self.NOISE_SAVE = args['NOISE_SAVE']\r\n        self.SAVE_A1 = args['SAVE_A1']\r\n        self.SAVE_B1 = args['SAVE_B1']\r\n        self.SAVE_C1 = args['SAVE_C1']\r\n\r\n        self.OPTI1 = args['SAVE_OPTI1']\r\n        self.OPTI2 = args['SAVE_OPTI2']\r\n   \r\n    def _create_koopman_matrix_a1(self, args):\r\n        \"\"\"\r\n        In this approach\r\n        A,B,C,D are regarded as the same as \r\n        the parameters in neural networks\r\n        \"\"\"\r\n        scale = torch.tensor(0.01)\r\n        \r\n        self.A_1 = torch.randn(args['latent_dim'], args['latent_dim']).to(args['device'])\r\n        self.B_1 = torch.randn(args['act_dim'], args['latent_dim']).to(args['device'])\r\n        self.C_1 = torch.randn(args['latent_dim'], args['state_dim']).to(args['device'])\r\n\r\n        self.A_1 = scale*self.A_1\r\n        self.A_1.requires_grad_(True)\r\n\r\n        self.B_1 = scale*self.B_1\r\n        self.B_1.requires_grad_(True)\r\n\r\n        self.C_1 = scale*self.C_1\r\n        self.C_1.requires_grad_(True)\r\n    \r\n\r\n\r\n    def _create_koopman_matrix_a1_extend(self, args):\r\n        #用这个\r\n        \"\"\"\r\n        In this approach\r\n        A,B,C,D are regarded as the same as \r\n        the parameters in neural networks\r\n        \"\"\"\r\n        scale = torch.tensor(0.01)\r\n        \r\n        self.A_1 = torch.randn(args['latent_dim']+args['state_dim'], args['latent_dim']+args['state_dim']).to(args['device'])\r\n        self.B_1 = torch.randn(args['act_expand'], args['latent_dim']+args['state_dim']).to(args['device'])\r\n        self.C_1 = torch.zeros(args['latent_dim']+args['state_dim'], args['state_dim']).to(args['device'])\r\n        # self.C_2 = torch.randn(args['latent_dim']+args['state_dim'], args['output_dim']).to(args['device'])\r\n\r\n        self.A_1 = scale * self.A_1+torch.eye(args['latent_dim']+args['state_dim'], args['latent_dim']+args['state_dim']).to(args['device'])\r\n        self.A_1.requires_grad_(True)\r\n\r\n        self.B_1 = scale*self.B_1\r\n        self.B_1.requires_grad_(True)\r\n\r\n        # C does not to be learned. Select as [I,0]\r\n        # self.C_1 = scale*self.C_1\r\n        # self.C_1[:args['state_dim'],:]= (1.0*torch.eye(args['state_dim'],args['state_dim'])\\\r\n        #                                  +torch.randn(args['state_dim'],args['state_dim'])*scale).to(args['device'])\r\n        # self.C_1.requires_grad_(True)\r\n\r\n        self.C_1[:args['state_dim'],:] = torch.eye(args['state_dim'],args['state_dim'])\r\n\r\n\r\n\r\n    def _create_koopman_matrix_a2(self, args):\r\n        \"\"\"\r\n        In this approach\r\n        A,B,C,D will be solved by traditional \r\n        system analysis method\r\n\r\n        TODO: Initialization of A B and C\r\n        \"\"\" \r\n        self.A_1 = torch.randn(args['latent_dim'], args['latent_dim']).to(args['device'])\r\n        self.B_1 = torch.randn(args['act_dim'], args['latent_dim']).to(args['device'])\r\n        self.C_1 = torch.randn(args['latent_dim'], args['state_dim']).to(args['device'])\r\n\r\n    def _create_koopman_matrix_a2_extend(self, args):\r\n        \"\"\"\r\n        In this approach\r\n        A,B,C,D are regarded as the same as \r\n        the parameters in neural networks\r\n        \"\"\"\r\n        \r\n        self.A_1 = torch.randn(args['latent_dim']+args['state_dim'], args['latent_dim']+args['state_dim']).to(args['device'])\r\n        self.B_1 = torch.randn(args['act_dim'], args['latent_dim']+args['state_dim']).to(args['device'])\r\n        self.C_1 = torch.randn(args['latent_dim']+args['state_dim'], args['state_dim']).to(args['device'])\r\n\r\n    def _create_optimizer(self, args):\r\n        pass\r\n\r\n    def learn(self, e, x_train,x_val,shift,args):\r\n        self.train_data = DataLoader(dataset = x_train, batch_size = args['batch_size'], shuffle = True, drop_last = True)\r\n        count = 0\r\n        \r\n        for x_,u_ in self.train_data:\r\n            self.loss = 0\r\n            self.d_loss = 0\r\n            self.p2_loss = 0\r\n            self.p3_loss = 0\r\n            x_=x_.to(args['device'])\r\n            u_=u_.to(args['device'])\r\n            self.pred_forward(x_,u_,shift,args)\r\n            count += 1\r\n\r\n            self.optimizer1.zero_grad()\r\n            if args['ABCD'] != 2:\r\n                self.optimizer2.zero_grad()\r\n            self.loss.backward()\r\n            self.optimizer1.step()\r\n            self.optimizer1_sch.step()\r\n            self.optimizer3.step()\r\n            self.optimizer3_sch.step()\r\n\r\n\r\n        if args['ABCD'] != 2:\r\n            self.optimizer2.step()\r\n            # self.optimizer2.step()\r\n            self.optimizer2_sch.step()\r\n\r\n        if args['if_pi']:\r\n            loss_buff = self.d_loss / count\r\n        else:\r\n            loss_buff = self.loss / count\r\n\r\n        if loss_buff<self.loss_buff:\r\n            # self.C_1_restore = self.C_1\r\n            self.net_para = copy.deepcopy(self.net.state_dict())\r\n            self.noise_para = copy.deepcopy(self.noisemlp.state_dict())\r\n            self.A_1_restore = copy.deepcopy(self.A_1)\r\n            self.B_1_restore = copy.deepcopy(self.B_1)\r\n            self.C_1_restore = copy.deepcopy(self.C_1)\r\n            self.loss_buff = loss_buff\r\n        #validation_test\r\n        \r\n        self.loss = 0\r\n        self.d_loss = 0\r\n        self.p1_loss = 0\r\n        self.p2_loss = 0\r\n        self.p3_loss = 0\r\n        count = 0\r\n        self.val_data = DataLoader(dataset = x_val, batch_size = args['batch_size'], shuffle = True, drop_last = True)\r\n\r\n        for x_,u_ in self.val_data:\r\n            x_=x_.to(args['device'])\r\n            u_=u_.to(args['device'])\r\n            self.pred_forward(x_,u_,shift,args)\r\n            count += 1\r\n\r\n        if args['if_pi']:\r\n            print('epoch {}: loss_traning data {} loss_val data {} minimal loss {} learning_rate {}'.format(e,loss_buff,self.d_loss/count,self.loss_buff,self.optimizer1_sch.get_last_lr()))\r\n            return loss_buff,self.d_loss / count\r\n\r\n        else:\r\n            print(\r\n                'epoch {}: loss_traning data {} loss_val data {} minimal loss {} learning_rate {}'.format(e, loss_buff,\r\n                                                                                                          self.loss / count,\r\n                                                                                                          self.loss_buff,\r\n                                                                                                          self.optimizer1_sch.get_last_lr()))\r\n            return loss_buff, self.loss / count\r\n\r\n    def test_(self,test,args):\r\n        self.test_data = DataLoader(dataset = test, batch_size = 10, shuffle = True)\r\n        for x_,u_ in self.test_data:\r\n            x_=x_.to(args['device'])\r\n            u_=u_.to(args['device'])\r\n            self.pred_forward_test(x_,u_,args)\r\n\r\n\r\n    def pred_forward(self,x,u,shift,args):\r\n        pred_horizon = args['pred_horizon']\r\n        x0_buff = x[:,0,:]\r\n        x0 = self.net(x0_buff)\r\n        x_pred_all = self.net(x)[:,1:,:]\r\n\r\n        loss = nn.MSELoss()\r\n\r\n        if args['extend_state']:\r\n            x0 = torch.cat([x0_buff,x0],1)\r\n\r\n        # if args['act_expand'] > args['act_dim']:\r\n        #     if args['act_expand']  == 6:\r\n        #         u = torch.cat([torch.square(u),u],2)\r\n        #     if args['act_expand']  == 9:\r\n        #         u = torch.cat([torch.pow(u,3),torch.square(u),u],2)\r\n\r\n        if args['ABCD'] == 2:\r\n            x1_buff = x[:,1,:]\r\n            x1 = self.net(x1_buff)\r\n            if args['extend_state']:\r\n                x1 = torch.cat([x1_buff,x1],1)\r\n                u0 = u[:,0,:]\r\n                x_all = torch.cat([x0,u0],1)\r\n                K = torch.linalg.lstsq(x_all,x1).solution\r\n                self.A_1 = K[:args['state_dim']+args['latent_dim'],:]\r\n                self.B_1 = K[-args['act_dim']:,:]\r\n\r\n\r\n        x_pred_matrix = torch.zeros_like(x[:,1:,:])\r\n        # y_pred_matrix = torch.zeros_like(x[:, 1:, :])\r\n        x_pred_matrix_all = torch.zeros([x.shape[0],x.shape[1]-1,args['latent_dim']]).to(args['device'])\r\n        x_pred_matrix_n = torch.zeros_like(x[:, 1:, :])\r\n        x_pred_matrix_all_n = torch.zeros([x.shape[0], x.shape[1] - 1, args['latent_dim']]).to(args['device'])\r\n\r\n        self.w_mean = torch.zeros(args['batch_size'], args['state_dim'] + args['latent_dim']).to(args['device'])\r\n        base_distribution = dist.MultivariateNormal(torch.zeros(args['latent_dim'] + args['state_dim']),\r\n                                                    torch.eye(args['latent_dim'] + args['state_dim']))\r\n        self.epsilon = base_distribution.sample((args['batch_size'],)).to(args['device'])\r\n\r\n        SCALE_DIAG_MIN_MAX = (-20, 2)\r\n        if args['if_sigma']: # Trained with the noise characteristic network\r\n            x0_n = x0\r\n            # assume sigma remain the same within one forward prediction window\r\n            log_sigma = self.noisemlp(x0_n)\r\n            log_sigma = torch.clamp(log_sigma, min=SCALE_DIAG_MIN_MAX[0], max=SCALE_DIAG_MIN_MAX[1])\r\n            self.sigma = torch.exp(log_sigma)\r\n            bijector = transforms.AffineTransform(loc=self.w_mean, scale=self.sigma)\r\n            self.w = bijector(self.epsilon)\r\n            for i in range(pred_horizon-1):\r\n                # self.sigma = torch.exp(log_sigma).unsqueeze(1)\r\n                # bijector = transforms.AffineTransform(loc=self.w_mean, scale=self.sigma)\r\n                # self.w = bijector(self.epsilon[:, :, i, :])\r\n                # # self.w = self.w_mean + self.e_sigma * self.epsilon[:, :, i]\r\n                # self.w = self.w.squeeze()\r\n                x0_n = torch.matmul(x0_n,self.A_1)+torch.matmul(u[:,i,:],self.B_1) + self.w\r\n                x_pred_n = torch.matmul(x0_n, self.C_1)\r\n                x_pred_matrix_all_n[:,i,:] = x0_n[:, -args['latent_dim']:]\r\n                x_pred_matrix_n[:,i,:] = x_pred_n\r\n\r\n        for i in range(pred_horizon-1):\r\n            x0 = torch.matmul(x0,self.A_1)+torch.matmul(u[:,i,:],self.B_1)\r\n            x_pred = torch.matmul(x0,self.C_1)\r\n            x_pred_matrix_all[:,i,:] = x0[:, -args['latent_dim']:]\r\n            x_pred_matrix[:,i,:] = x_pred\r\n\r\n        # #------------------------------#\r\n        # self.d_loss += (loss(x_pred_matrix,x[:,1:,:])*10)\r\n        # self.d_loss += loss(x_pred_all.to(args['device']),x_pred_matrix_all.to(args['device']))\r\n        # #-----------terminal constraints-----------##\r\n        # self.d_loss += loss(x_pred_matrix[:,-1,:],x[:,-1,:])*10\r\n        # self.d_loss += loss(x_pred_all[:,-1,:].to(args['device']),x_pred_matrix_all[:,-1,:].to(args['device']))\r\n        # ------------------------------#\r\n\r\n        self.select = [2, 5, 8]\r\n        d_loss, p2_loss, p3_loss = 0., 0., 0.\r\n        if args['if_sigma']:\r\n            self.d_loss += loss(x_pred_matrix_n[:, :, :], x[:, 1:, :]) * 10\r\n            self.d_loss += loss(x_pred_all[:, :, :], x_pred_matrix_all_n[:, :, :])\r\n            # -----------terminal constraints-----------##\r\n            self.d_loss += loss(x_pred_matrix_n[:, -1, :], x[:, -1, :]) * 10\r\n            self.d_loss += loss(x_pred_all[:, -1, :], x_pred_matrix_all_n[:, -1, :])\r\n        else:\r\n            self.d_loss += loss(x_pred_matrix[:, :, :], x[:, 1:, :]) * 10\r\n            self.d_loss += loss(x_pred_all[:, :, :], x_pred_matrix_all[:, :, :])\r\n            # -----------terminal constraints-----------##\r\n            self.d_loss += loss(x_pred_matrix[:, -1, :], x[:, -1, :]) * 10\r\n            self.d_loss += loss(x_pred_all[:, -1, :], x_pred_matrix_all[:, -1, :])\r\n\r\n        if args['if_pi']:\r\n            ##########################\r\n            # --------------physics informed----------------- #\r\n            # system = CSTR_system()\r\n            # system = three_tank_system(args)\r\n            system = physics(args)\r\n            x_pred_matrix_re = x_pred_matrix * shift[1] + shift[0]\r\n            u_re = u * shift[3] + shift[2]\r\n\r\n            # ############################\r\n            '''\r\n            loss2 : X_k+1 - X_k = ΔX_k\r\n            '''\r\n            dxk_s = system.derivative(x_pred_matrix_re[:, :-1, :], u_re[:, 1:, :]) * system.h\r\n            dxk = (dxk_s - shift[0]) / shift[1]\r\n            pred_dxk = x_pred_matrix[:, 1:, :] - x_pred_matrix[:, :-1, :]\r\n            # self.p2_loss += 0.5 * loss(dxk[:, :, :], pred_dxk[:, :, :])\r\n            self.p2_loss += 0.1 * loss(dxk[:, :, self.select], pred_dxk[:, :, self.select])\r\n\r\n            ############################\r\n            '''\r\n            loss3 partial: g(xk+1) = g(f(xk,uk))\r\n            '''\r\n            xk = torch.zeros([args['batch_size'], args['pred_horizon'] - 2, args['state_dim']]).to(args['device'])\r\n            # dxk_s = system.derivative(x_pred_matrix_re[:, :-1, :], u_re[:, 1:, :]) * system.h\r\n            xk[:, :, :] = x_pred_matrix_re[:, 1:, :]\r\n            xk[:, :, self.select] = dxk_s[:, :, self.select] + x_pred_matrix_re[:, :-1, self.select]\r\n            xk = (xk - shift[0]) / shift[1]\r\n            gxk = self.net(xk)  # g(f(xk,uk))\r\n            pred_gxk = x_pred_matrix_all[:, 1:, :]  # g(xk+1)\r\n            self.p3_loss += 100 * loss(pred_gxk, gxk)\r\n\r\n            # self adaptive loss\r\n            # self.loss = (1/pow(self.d, 2))*self.d_loss \\\r\n            #             + (1/pow(self.p2, 2))*self.p2_loss\\\r\n            #             + 200 * (torch.log(1 + pow(self.d, 2)) + torch.log(1 + pow(self.p2, 2)))\r\n            self.loss = (1/pow(self.d, 2))*self.d_loss \\\r\n                        + (1/pow(self.p2, 2))*self.p2_loss\\\r\n                        + (1/pow(self.p3, 2))*self.p3_loss\\\r\n                        + 200 * (torch.log(1 + pow(self.d, 2)) + torch.log(1 + pow(self.p2, 2)) + torch.log(1 + pow(self.p3, 2)))\r\n\r\n        else:\r\n            self.loss = self.d_loss\r\n\r\n\r\n        self.displace1 = x_pred[7,:]\r\n        self.displace2 = x[7, i+1, :]\r\n\r\n\r\n    def pred_forward_test(self,x,u,shift,test,args,e=0,test_save=True):\r\n        x = x.to(args['device'])\r\n        u = u.to(args['device'])\r\n        self.test_loss = 0\r\n        count = 0\r\n        x_pred_list = []\r\n        x_sum_list = []\r\n        x_real_list = []\r\n        x_time_list = []\r\n        plt.close()\r\n        f, axs = plt.subplots(args['state_dim'], sharex=True, figsize=(15, 15))\r\n        time_all = torch.arange(x.shape[1])\r\n\r\n        if test:\r\n            # for i in range(0,args['max_ep_steps']-args['pred_horizon'],10):\r\n            for i in range(0, args['max_ep_steps'] * args['test_steps'] - args['pred_horizon'] + 1,\r\n                           args['pred_horizon'] - 1):\r\n                x_pred = x[:,i:i+args['pred_horizon']]\r\n                u_pred = u[:,i:i+args['pred_horizon']]\r\n                x_pred_list_buff,x_real_list_buff,x_sum_list_buff,loss_test = \\\r\n                    Koopman_Desko.pred_forward_test_buff(self,x_pred,u_pred,args)\r\n                x_pred_list.append(torch.tensor(x_pred_list_buff))\r\n                x_real_list.append(torch.tensor(x_real_list_buff))\r\n                x_sum_list.append(torch.tensor(x_sum_list_buff))\r\n                x_time_list.append(torch.arange(i+1,i+args['pred_horizon']))\r\n                self.test_loss += loss_test\r\n                count += 1\r\n            self.test_loss = self.test_loss/count\r\n\r\n            print(\"test_loss{}\".format(self.test_loss))\r\n            ## scale back\r\n            if (e % 50 == 0) and args['plot_test']:\r\n                x = x * shift[1] + shift[0]\r\n                x_pred_list = torch.stack(x_pred_list).to(args['device']) * shift[1] + shift[0]\r\n                x=x.squeeze().cpu()\r\n                x_pred_list=x_pred_list.cpu()\r\n\r\n                x_pred_list = np.array(x_pred_list)\r\n                x = np.array(x)\r\n                x_time_list = np.array(x_time_list)\r\n\r\n             ##------------------- plot -----------------##\r\n                color1 = \"#038355\"  # 孔雀绿\r\n                font = {'family': 'Times New Roman', 'size': 12}\r\n                titles = ['XA1', 'XB1', 'T1',\r\n                          'XA2', 'XB2', 'T2',\r\n                          'XA3', 'XB3', 'T3']\r\n                plt.rc('font', **font)\r\n                f, axs = plt.subplots(3, 3, sharex=True, figsize=(15, 9))\r\n                legend_created = False\r\n                for i in range(3):  # 行索引\r\n                    for j in range(3):  # 列索引\r\n                        # 绘制每个子图\r\n                        axs[i, j].plot(x[:, i * 3 + j], label='Ground Truth', color='k', linewidth=2)\r\n                        for k in range(len(x_time_list)):\r\n                            axs[i, j].plot(x_time_list[k], x_pred_list[k][:, :, i * 3 + j], label='Koopman Model Prediction', color=color1, linewidth=2)\r\n                            if not legend_created:  # 如果还没有获取过 handles 和 labels\r\n                                handles, labels = axs[i, j].get_legend_handles_labels()\r\n                                legend_created = True  # 设置标志变量为 True，表示已经获取过一次\r\n                        axs[i, j].set_title(titles[i * 3 + j])\r\n                        axs[i, j].legend().set_visible(False)\r\n\r\n                for ax in axs[-1, :]:\r\n                    ax.set_xlabel('Time Steps')\r\n                plt.tight_layout()\r\n                f.legend(handles, labels, loc='lower center', ncol=len(handles), bbox_to_anchor=(0.5, 0.02))\r\n                plt.subplots_adjust(bottom=0.15)\r\n                if test_save:\r\n                    if args['if_pi']:\r\n                        result_type = 'pi'\r\n                    else:\r\n                        result_type = 'nopi'\r\n                    plt.savefig('open_loop_result/'+str(result_type)+'/predictions_new' + str(e) + '.pdf')\r\n                    # plt.savefig('open_loop_result/predictions_new' + str(e) + '.png')\r\n                    print(\"plot\")\r\n                    print(\"save test list\")\r\n                    torch.save(x_pred_list, 'open_loop_result/'+str(result_type)+'/x_pred_list.pt')\r\n                    torch.save(x, 'open_loop_result/'+str(result_type)+'/x.pt')\r\n                    torch.save(x_time_list, 'open_loop_result/'+str(result_type)+'/x_time_list.pt')\r\n\r\n            return x_pred_list,x_real_list,x_sum_list,self.test_loss\r\n\r\n\r\n        ##----------------------------------------------##\r\n        else:\r\n            return Koopman_Desko.pred_forward_test_buff(self,x,u,args)\r\n    \r\n    def pred_forward_test_buff(self,x,u,args):\r\n        pred_horizon = args['pred_horizon']\r\n\r\n        #测试模式\r\n        self.net.eval()\r\n\r\n        x0_buff = x[:,0,:]        \r\n        x0 = self.net(x0_buff)\r\n\r\n\r\n        if args['extend_state']:\r\n            x0 = torch.cat([x0_buff,x0],1)\r\n\r\n        if args['ABCD'] == 2:\r\n            x1_buff = x[:,1,:]\r\n            x1 = self.net(x1_buff)\r\n            if args['extend_state']:\r\n                x1 = torch.cat([x1_buff,x1],1)\r\n                u0 = u[:,0,:]\r\n                x_all = torch.cat([x0,u0],1)\r\n                K = torch.linalg.lstsq(x_all,x1)\r\n                print(\"try\")\r\n\r\n        x_pred_list = []\r\n        x_sum_list = []\r\n        x_real_list = []\r\n\r\n        loss_test = 0\r\n        loss = nn.MSELoss()\r\n\r\n        x_pred_all = self.net(x[:,:-1,:])\r\n\r\n        x_pred_matrix = torch.zeros_like(x[:,1:,:])\r\n        for i in range(pred_horizon-1):\r\n            x0 = torch.matmul(x0,self.A_1)+torch.matmul(u[:,i,:],self.B_1)\r\n            x_pred = torch.matmul(x0, self.C_1)\r\n            # if args['extend_state']:\r\n            #     x_pred = torch.matmul(x0,self.C_1)\r\n            # else:\r\n            #     x_pred = torch.matmul(x0,self.C_1)\r\n\r\n            # x_pred = torch.matmul(x0,self.C_1)\r\n            x_pred_matrix[:,i,:] = x_pred\r\n            x_pred_list.append(x_pred.cpu().detach().numpy())\r\n            x_real_list.append(x[:,i+1,:].cpu().detach().numpy())\r\n\r\n        loss_test += loss(x_pred_matrix, x[:, 1:, :])\r\n\r\n        self.net.train()\r\n\r\n        return x_pred_list,x_real_list,x_sum_list,loss_test\r\n\r\n\r\n\r\n    def parameter_store(self,args):\r\n        #save nn\r\n        torch.save(self.net_para,self.MODEL_SAVE)\r\n        #save noise sigma\r\n        torch.save(self.noise_para, self.NOISE_SAVE)\r\n        #save A1 B1 C1\r\n        torch.save(self.A_1_restore,self.SAVE_A1)\r\n        torch.save(self.B_1_restore,self.SAVE_B1)\r\n        torch.save(self.C_1_restore,self.SAVE_C1)\r\n\r\n        torch.save(self.optimizer1.state_dict(),self.OPTI1)\r\n        if args['ABCD'] != 2:\r\n            torch.save(self.optimizer2.state_dict(),self.OPTI2)\r\n\r\n        print(\"store!!!\")\r\n\r\n        \r\n        \r\n    def parameter_restore(self,args):\r\n\r\n        # self.A_1 = torch.load(self.SAVE_A1, map_location=torch.device(args['device']))\r\n        self.A_1 = torch.load(self.SAVE_A1, map_location='cpu')\r\n        self.B_1 = torch.load(self.SAVE_B1, map_location='cpu')\r\n        self.C_1 = torch.load(self.SAVE_C1, map_location='cpu')\r\n\r\n        self.net = MLP(args)\r\n        self.net.load_state_dict(torch.load(self.MODEL_SAVE, map_location='cpu'))\r\n        self.net.eval()\r\n\r\n        if args['if_sigma']:\r\n            self.noisemlp = Noise_MLP(args)\r\n            self.noisemlp.load_state_dict(torch.load(self.NOISE_SAVE, map_location='cpu'))\r\n            self.noisemlp.eval()\r\n        print(\"restore!\")\r\n\r\n\r\n    def set_shift_and_scale(self, replay_memory):\r\n\r\n        self.shift = replay_memory.shift_[0]\r\n        self.scale = replay_memory.shift_[1]\r\n        self.shift_u = replay_memory.shift_[2]\r\n        self.scale_u = replay_memory.shift_[3]\r\n\r\n    def _create_optimizer(self, args):\r\n        pass\r\n\r\n\r\nclass physics():\r\n    def __init__(self, args):\r\n        self.args = args\r\n        self.h = torch.tensor(0.001).to(self.args['device'])\r\n\r\n        self.s2hr = torch.tensor(3600).to(self.args['device'])\r\n        self.MW = torch.tensor(250e-3).to(self.args['device'])\r\n        self.sum_c = torch.tensor(2E3).to(self.args['device'])\r\n        self.T10 = torch.tensor(300).to(self.args['device'])\r\n        self.T20 = torch.tensor(300).to(self.args['device'])\r\n        self.F10 = torch.tensor(5.04).to(self.args['device'])\r\n        self.F20 = torch.tensor(5.04).to(self.args['device'])\r\n        self.Fr = torch.tensor(50.4).to(self.args['device'])\r\n        self.Fp = torch.tensor(0.504).to(self.args['device'])\r\n        self.V1 = torch.tensor(1).to(self.args['device'])\r\n        self.V2 = torch.tensor(0.5).to(self.args['device'])\r\n        self.V3 = torch.tensor(1).to(self.args['device'])\r\n        self.E1 = torch.tensor(5e4).to(self.args['device'])\r\n        self.E2 = torch.tensor(6e4).to(self.args['device'])\r\n        self.k1 = torch.tensor(2.77e3).to(self.args['device']) * self.s2hr\r\n        self.k2 = torch.tensor(2.6e3).to(self.args['device']) * self.s2hr\r\n        self.dH1 = -torch.tensor(6e4).to(self.args['device']) / self.MW\r\n        self.dH2 = -torch.tensor(7e4).to(self.args['device']) / self.MW\r\n        self.aA = torch.tensor(3.5).to(self.args['device'])\r\n        self.aB = torch.tensor(1).to(self.args['device'])\r\n        self.aC = torch.tensor(0.5).to(self.args['device'])\r\n        self.Cp = torch.tensor(4.2e3).to(self.args['device'])\r\n        self.R = torch.tensor(8.314).to(self.args['device'])\r\n        self.rho = torch.tensor(1000).to(self.args['device'])\r\n        self.xA10 = torch.tensor(1).to(self.args['device'])\r\n        self.xB10 = torch.tensor(0).to(self.args['device'])\r\n        self.xA20 = torch.tensor(1).to(self.args['device'])\r\n        self.xB20 = torch.tensor(0).to(self.args['device'])\r\n        self.Hvap1 = -torch.tensor(35.3E3).to(self.args['device']) * self.sum_c\r\n        self.Hvap2 = -torch.tensor(15.7E3).to(self.args['device']) * self.sum_c\r\n        self.Hvap3 = -torch.tensor(40.68E3).to(self.args['device']) * self.sum_c\r\n\r\n        self.noise_error_std = torch.tensor([0.001, 0.001, 0.01, 0.001, 0.001, 0.01, 0.001, 0.001, 0.01]).to(\r\n            self.args['device'])\r\n        self.noise_error_clip = torch.tensor([0.01, 0.01, 0.1, 0.01, 0.01, 0.1, 0.01, 0.01, 0.1]).to(\r\n            self.args['device'])\r\n        # self.kw = torch.tensor([0.01, 0.01, 0.1, 0.01, 0.01, 0.1, 0.01, 0.01, 0.1]).to(self.args['device'])\r\n        # self.kw = torch.tensor([0.01, 0.01, 0.1, 0.01, 0.01, 0.1, 0.01, 0.01, 0.1]).to(self.args['device'])+\\\r\n        #           torch.clamp(torch.normal(mean=0, std=self.noise_error_std), -self.noise_error_clip, self.noise_error_clip) # noise deviation\r\n        self.kw = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]).to(self.args['device'])+ \\\r\n                  torch.clamp(torch.normal(mean=0, std=self.noise_error_std), -self.noise_error_clip,\r\n                              self.noise_error_clip)  # noise deviation\r\n        self.bw = torch.tensor([0.1, 0.1, 1, 0.1, 0.1, 1, 0.1, 0.1, 1]).to(self.args['device'])  # noise bound\r\n    def random_noise(self,x):\r\n        noise = torch.zeros_like(x)\r\n        for i in range(x.shape[0]):\r\n            for j in range(x.shape[1]):\r\n                process_noise = torch.normal(mean=0, std=self.kw)\r\n                process_noise = torch.clamp(process_noise, -self.bw, self.bw)\r\n                noise[i, j, :] = process_noise\r\n        return noise\r\n    def derivative(self, x, us):\r\n        xA1 = x[:,:,0]\r\n        xB1 = x[:,:,1]\r\n        T1 = x[:,:,2]\r\n\r\n        xA2 = x[:,:,3]\r\n        xB2 = x[:,:,4]\r\n        T2 = x[:,:,5]\r\n\r\n        xA3 = x[:,:,6]\r\n        xB3 = x[:,:,7]\r\n        T3 = x[:,:,8]\r\n\r\n        Q1 = us[:,:,0]\r\n        Q2 = us[:,:,1]\r\n        Q3 = us[:,:,2]\r\n\r\n        xC3 = 1 - xA3 - xB3\r\n        x3a = self.aA * xA3 + self.aB * xB3 + self.aC * xC3\r\n\r\n        xAr = self.aA * xA3 / x3a\r\n        xBr = self.aB * xB3 / x3a\r\n        xCr = self.aC * xC3 / x3a\r\n\r\n        F1 = self.F10 + self.Fr\r\n        F2 = F1 + self.F20\r\n        # F3 = F2 - self.Fr - self.Fp\r\n\r\n        f1 = self.F10 * (self.xA10 - xA1) / self.V1 + self.Fr * (xAr - xA1) / self.V1 - self.k1 * torch.exp(\r\n            -self.E1 / (self.R * T1)) * xA1\r\n        f2 = self.F10 * (self.xB10 - xB1) / self.V1 + self.Fr * (xBr - xB1) / self.V1 + self.k1 * torch.exp(\r\n            -self.E1 / (self.R * T1)) * xA1 - self.k2 * torch.exp(\r\n            -self.E2 / (self.R * T1)) * xB1\r\n        f3 = self.F10 * (self.T10 - T1) / self.V1 + self.Fr * (T3 - T1) / self.V1 - self.dH1 * self.k1 * torch.exp(\r\n            -self.E1 / (self.R * T1)) * xA1 / self.Cp - self.dH2 * self.k2 * torch.exp(\r\n            -self.E2 / (self.R * T1)) * xB1 / self.Cp + Q1 / (self.rho * self.Cp * self.V1)\r\n\r\n        f4 = F1 * (xA1 - xA2) / self.V2 + self.F20 * (self.xA20 - xA2) / self.V2 - self.k1 * torch.exp(\r\n            -self.E1 / (self.R * T2)) * xA2\r\n        f5 = F1 * (xB1 - xB2) / self.V2 + self.F20 * (self.xB20 - xB2) / self.V2 + self.k1 * torch.exp(\r\n            -self.E1 / (self.R * T2)) * xA2 - self.k2 * torch.exp(\r\n            -self.E2 / (self.R * T2)) * xB2\r\n        f6 = F1 * (T1 - T2) / self.V2 + self.F20 * (self.T20 - T2) / self.V2 - self.dH1 * self.k1 * torch.exp(\r\n            -self.E1 / (self.R * T2)) * xA2 / self.Cp - self.dH2 * self.k2 * torch.exp(\r\n            -self.E2 / (self.R * T2)) * xB2 / self.Cp + Q2 / (self.rho * self.Cp * self.V2)\r\n\r\n        f7 = F2 * (xA2 - xA3) / self.V3 - (self.Fr + self.Fp) * (xAr - xA3) / self.V3\r\n        f8 = F2 * (xB2 - xB3) / self.V3 - (self.Fr + self.Fp) * (xBr - xB3) / self.V3\r\n        f9 = F2 * (T2 - T3) / self.V3 + Q3 / (self.rho * self.Cp * self.V3) + (self.Fr + self.Fp) * (\r\n                    xAr * self.Hvap1 + xBr * self.Hvap2 + xCr * self.Hvap3) / (\r\n                     self.rho * self.Cp * self.V3)\r\n        F = torch.stack([f1, f2, f3, f4, f5, f6, f7, f8, f9])\r\n        F = F.permute(1, 2, 0)\r\n        return F\r\n\r\n# class encoder(nn.Module):\r\n#     def __init__(self, input_dim, hid_dim, n_layers, dropout):\r\n#         super().__init__()\r\n#         self.hid_dim = hid_dim\r\n#         self.n_layers = n_layers\r\n#\r\n#         self.rnn = nn.LSTM(input_dim, hid_dim, n_layers, dropout = dropout)\r\n#         self.dropout = nn.Dropout(dropout)\r\n#\r\n#     def forward(self, input):\r\n#         outputs, (hidden, cell) = self.rnn(input)\r\n#         return outputs ,hidden, cell\r\n\r\nclass MLP(nn.Module):\r\n    def __init__(self, args):\r\n        super(MLP, self).__init__()\r\n        self.model = nn.Sequential(\r\n            nn.Linear(args['state_dim'], 256),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.5),\r\n            nn.Linear(256, 128),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.5),\r\n            # nn.Linear(100, 80),\r\n            nn.Linear(128, args['latent_dim']),\r\n            # nn.ReLU(),\r\n            # nn.Dropout(0.5),\r\n            # nn.Linear(80, args['latent_dim']),\r\n            # nn.ReLU(),\r\n        )\r\n    def forward(self,x):\r\n        return self.model(x)\r\n\r\n\r\nclass Noise_MLP(nn.Module):\r\n    def __init__(self, args):\r\n        super(Noise_MLP, self).__init__()\r\n        self.model = nn.Sequential(\r\n            nn.Linear(args['state_dim']+args['latent_dim'], 128),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.1),\r\n            nn.Linear(128, 64),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.1),\r\n            # nn.Linear(100, 80),\r\n            nn.Linear(64, args['latent_dim']+args['state_dim']),\r\n        )\r\n    def forward(self,x):\r\n        return self.model(x)\r\n\r\nif __name__ == '__main__':\r\n    pass    \r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Desko.py b/Desko.py
--- a/Desko.py	(revision 7ab1852fc127f85716683578724a4afd6f5e8c57)
+++ b/Desko.py	(date 1719541118398)
@@ -211,10 +211,8 @@
             self.optimizer3.step()
             self.optimizer3_sch.step()
 
-
         if args['ABCD'] != 2:
             self.optimizer2.step()
-            # self.optimizer2.step()
             self.optimizer2_sch.step()
 
         if args['if_pi']:
@@ -223,38 +221,37 @@
             loss_buff = self.loss / count
 
         if loss_buff<self.loss_buff:
-            # self.C_1_restore = self.C_1
-            self.net_para = copy.deepcopy(self.net.state_dict())
-            self.noise_para = copy.deepcopy(self.noisemlp.state_dict())
-            self.A_1_restore = copy.deepcopy(self.A_1)
-            self.B_1_restore = copy.deepcopy(self.B_1)
-            self.C_1_restore = copy.deepcopy(self.C_1)
-            self.loss_buff = loss_buff
+                # self.C_1_restore = self.C_1
+                self.net_para = copy.deepcopy(self.net.state_dict())
+                self.noise_para = copy.deepcopy(self.noisemlp.state_dict())
+                self.A_1_restore = copy.deepcopy(self.A_1)
+                self.B_1_restore = copy.deepcopy(self.B_1)
+                self.C_1_restore = copy.deepcopy(self.C_1)
+                self.loss_buff = loss_buff
+
         #validation_test
-        
-        self.loss = 0
-        self.d_loss = 0
-        self.p1_loss = 0
-        self.p2_loss = 0
-        self.p3_loss = 0
-        count = 0
-        self.val_data = DataLoader(dataset = x_val, batch_size = args['batch_size'], shuffle = True, drop_last = True)
-
-        for x_,u_ in self.val_data:
+        count = 0
+        self.val_data = DataLoader(dataset = x_val, batch_size = args['batch_size'], shuffle = True, drop_last = True)
+
+        for x_,u_ in self.val_data:
+            self.loss = 0
+            self.d_loss = 0
+            self.p2_loss = 0
+            self.p3_loss = 0
             x_=x_.to(args['device'])
             u_=u_.to(args['device'])
             self.pred_forward(x_,u_,shift,args)
             count += 1
 
+
         if args['if_pi']:
-            print('epoch {}: loss_traning data {} loss_val data {} minimal loss {} learning_rate {}'.format(e,loss_buff,self.d_loss/count,self.loss_buff,self.optimizer1_sch.get_last_lr()))
+            print('epoch {}: loss_traning data {} loss_val data {} learning_rate {}'.format(e,loss_buff,self.d_loss/count,self.optimizer1_sch.get_last_lr()))
             return loss_buff,self.d_loss / count
 
         else:
             print(
-                'epoch {}: loss_traning data {} loss_val data {} minimal loss {} learning_rate {}'.format(e, loss_buff,
+                'epoch {}: loss_traning data {} loss_val data {} learning_rate {}'.format(e, loss_buff,
                                                                                                           self.loss / count,
-                                                                                                          self.loss_buff,
                                                                                                           self.optimizer1_sch.get_last_lr()))
             return loss_buff, self.loss / count
 
@@ -277,12 +274,6 @@
         if args['extend_state']:
             x0 = torch.cat([x0_buff,x0],1)
 
-        # if args['act_expand'] > args['act_dim']:
-        #     if args['act_expand']  == 6:
-        #         u = torch.cat([torch.square(u),u],2)
-        #     if args['act_expand']  == 9:
-        #         u = torch.cat([torch.pow(u,3),torch.square(u),u],2)
-
         if args['ABCD'] == 2:
             x1_buff = x[:,1,:]
             x1 = self.net(x1_buff)
@@ -332,13 +323,6 @@
             x_pred_matrix_all[:,i,:] = x0[:, -args['latent_dim']:]
             x_pred_matrix[:,i,:] = x_pred
 
-        # #------------------------------#
-        # self.d_loss += (loss(x_pred_matrix,x[:,1:,:])*10)
-        # self.d_loss += loss(x_pred_all.to(args['device']),x_pred_matrix_all.to(args['device']))
-        # #-----------terminal constraints-----------##
-        # self.d_loss += loss(x_pred_matrix[:,-1,:],x[:,-1,:])*10
-        # self.d_loss += loss(x_pred_all[:,-1,:].to(args['device']),x_pred_matrix_all[:,-1,:].to(args['device']))
-        # ------------------------------#
 
         self.select = [2, 5, 8]
         d_loss, p2_loss, p3_loss = 0., 0., 0.
@@ -388,18 +372,13 @@
             self.p3_loss += 100 * loss(pred_gxk, gxk)
 
             # self adaptive loss
-            # self.loss = (1/pow(self.d, 2))*self.d_loss \
-            #             + (1/pow(self.p2, 2))*self.p2_loss\
-            #             + 200 * (torch.log(1 + pow(self.d, 2)) + torch.log(1 + pow(self.p2, 2)))
             self.loss = (1/pow(self.d, 2))*self.d_loss \
                         + (1/pow(self.p2, 2))*self.p2_loss\
                         + (1/pow(self.p3, 2))*self.p3_loss\
                         + 200 * (torch.log(1 + pow(self.d, 2)) + torch.log(1 + pow(self.p2, 2)) + torch.log(1 + pow(self.p3, 2)))
-
         else:
             self.loss = self.d_loss
 
-
         self.displace1 = x_pred[7,:]
         self.displace2 = x[7, i+1, :]
 
Index: my_args.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>args = {\r\n    'batch_size': 256,\r\n    'pred_horizon': 20,\r\n    'mhe_horizon': 45,\r\n    # latent dimension\r\n    'latent_dim': 13,  # lifted dimension\r\n    'state_dim': 9,\r\n    'output_dim': 3,\r\n    'act_dim': 3,\r\n    'seed': 7,\r\n    # import data\r\n    'import_saved_data': False,\r\n    'continue_data_collection': False,  # if True: continue training\r\n\r\n    'total_data_size': 2000,\r\n\r\n    'max_ep_steps': 300,\r\n    'test_steps': 10,\r\n    'max_test_ep_steps': 100,\r\n    'mhe_test_length': 20,\r\n    'ABCD': 1,\r\n    'val_frac': 0.2,\r\n    'lr1': 0.01,\r\n    'lr2': 0.001,\r\n    'lr3': 0.01,\r\n    'gamma': 0.8,\r\n    'mix_x_u': 3,\r\n    'if_mix': False,\r\n    'num_epochs': 251,\r\n    # 'num_epochs': 50,\r\n    'weight_decay': 10,\r\n\r\n    'extend_state': True,\r\n\r\n    'restore': False,\r\n\r\n    # 'if_pi': True,\r\n    'if_pi': False,\r\n    'reload_data': False,  # Use previous dataset\r\n    # 'reload_data': False,  # Regenerate new dataset\r\n    # 'if_sigma': True,\r\n    'if_sigma': False,\r\n\r\n    'plot_test': True,\r\n    'optimize_step': 20,\r\n    # 'act_expand': 1,  # do not expand the input\r\n\r\n    'MODEL_SAVE': \"save_model/model_v1.pt\",\r\n    'NOISE_SAVE': \"save_model/noise.pt\",\r\n    'SAVE_A1': \"save_model/A1.pt\",\r\n    'SAVE_B1': \"save_model/B1.pt\",\r\n    'SAVE_C1': \"save_model/C1.pt\",\r\n\r\n    # 'MODEL_SAVE': \"nopi_model/model_v1.pt\",\r\n    # 'NOISE_SAVE': \"nopi_model/noise.pt\",\r\n    # 'SAVE_A1': \"nopi_model/A1.pt\",\r\n    # 'SAVE_B1': \"nopi_model/B1.pt\",\r\n    # 'SAVE_C1': \"nopi_model/C1.pt\",\r\n\r\n    'SAVE_OPTI1': \"save_model/opti1_v1.pt\",\r\n    'SAVE_OPTI2': \"save_model/opti2_v1.pt\",\r\n    'SAVE_TEST': \"save_model/test.pt\",\r\n    'SAVE_TRAIN': \"save_model/train.pt\",\r\n    'SAVE_VAL': \"save_model/val.pt\",\r\n    'SAVE_SHIFT': \"save_model/shift.pt\",\r\n    'SAVE_TEST_X': \"save_model/test_x.pt\",\r\n    'SAVE_TEST_U': \"save_model/test_u.pt\",\r\n\r\n\r\n\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/my_args.py b/my_args.py
--- a/my_args.py	(revision 7ab1852fc127f85716683578724a4afd6f5e8c57)
+++ b/my_args.py	(date 1719541118400)
@@ -1,5 +1,5 @@
 args = {
-    'batch_size': 256,
+    'batch_size': 100,  # 256
     'pred_horizon': 20,
     'mhe_horizon': 45,
     # latent dimension
@@ -13,7 +13,6 @@
     'continue_data_collection': False,  # if True: continue training
 
     'total_data_size': 2000,
-
     'max_ep_steps': 300,
     'test_steps': 10,
     'max_test_ep_steps': 100,
@@ -26,7 +25,7 @@
     'gamma': 0.8,
     'mix_x_u': 3,
     'if_mix': False,
-    'num_epochs': 251,
+    'num_epochs': 151,
     # 'num_epochs': 50,
     'weight_decay': 10,
 
@@ -36,7 +35,7 @@
 
     # 'if_pi': True,
     'if_pi': False,
-    'reload_data': False,  # Use previous dataset
+    'reload_data': True,  # Use previous dataset
     # 'reload_data': False,  # Regenerate new dataset
     # 'if_sigma': True,
     'if_sigma': False,
Index: replay_memory.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import math\r\nimport numpy as np\r\nimport random\r\nimport progressbar\r\nimport os\r\nfrom torch.utils.data import Dataset, DataLoader, random_split\r\nimport torch\r\n\r\n\r\nclass MyDataSet(Dataset):\r\n\r\n    def __init__(self, test, x = None, y = None):\r\n        self.x = x\r\n        self.u = y\r\n        self.test = test\r\n\r\n\r\n\r\n    def __len__(self):\r\n        return len(self.x_choice)\r\n\r\n    def __getitem__(self, index):\r\n        return self.x_choice[index, :], self.y_choice[index]\r\n    \r\n    def determine_shift_and_scale(self, args):\r\n        self.shift_x = torch.mean(self.x, axis=(0, 1)).to(args['device'])\r\n        self.scale_x = torch.std(self.x, axis=(0, 1)).to(args['device'])\r\n        self.shift_u = torch.mean(self.u, axis=(0, 1)).to(args['device'])\r\n        self.scale_u = torch.std(self.u, axis=(0, 1)).to(args['device'])\r\n\r\n        return [self.shift_x,self.scale_x,self.shift_u,self.scale_u]\r\n    \r\n    def shift_scale(self, shift_ = None):\r\n\r\n        if self.test:\r\n            self.x_choice = (self.x - shift_[0])/shift_[1]\r\n            self.y_choice = (self.u - shift_[2])/shift_[3]\r\n        else:\r\n            self.x_choice = (self.x - self.shift_x)/self.scale_x\r\n            self.y_choice = (self.u - self.shift_u)/self.scale_u\r\n    \r\n    \r\n\r\n\r\n\r\n# Class to load and preprocess data\r\nclass ReplayMemory():\r\n    def __init__(self, args, env, predict_evolution = False, LSTM = False):\r\n        \"\"\"Constructs object to hold and update training/validation data.\r\n        Args:\r\n            args: Various arguments and specifications\r\n            shift: Shift of state values for normalization\r\n            scale: Scaling of state values for normalization\r\n            shift_u: Shift of action values for normalization\r\n            scale_u: Scaling of action values for normalization\r\n            env: Simulation environment\r\n            net: Neural network dynamics model\r\n            sess: TensorFlow session\r\n            predict_evolution: Whether to predict how system will evolve in time\r\n        \"\"\"\r\n        self.batch_size = args['batch_size']\r\n        self.seq_length = args['pred_horizon']\r\n        # self.shift_x = shift\r\n        # self.scale_x = scale\r\n        # self.shift_u = shift_u\r\n        # self.scale_u = scale_u\r\n        self.env = env\r\n        self.total_steps = 0\r\n        self.LSTM = LSTM\r\n\r\n        # print('validation fraction: ', args['val_frac'])\r\n\r\n\r\n        if args['import_saved_data'] or args['continue_data_collection']:\r\n            self._restore_data('./data/' + args['env_name'])\r\n            # self._process_data(args)\r\n\r\n            # print('creating splits...')\r\n            # self._create_split(args)\r\n            # self._determine_shift_and_scale(args)\r\n        else:\r\n            print(\"generating data...\")\r\n            self._generate_data(args)\r\n\r\n    def _generate_data(self, args):\r\n        \"\"\"Load data from environment\r\n        Args:\r\n            args: Various arguments and specifications\r\n        \"\"\"\r\n\r\n        # Initialize array to hold states and actions\r\n        x = []\r\n        u = []\r\n\r\n        # every trajectory reset the environment\r\n        # # Define progress bar\r\n        # bar = progressbar.ProgressBar(maxval=args['total_data_size']).start()\r\n        # length_list = []\r\n        # done_list = []\r\n        # # Loop through episodes\r\n        # while True:\r\n        #     # Define arrays to hold observed states and actions in each trial\r\n        #     x_trial = torch.zeros((args['max_ep_steps'], args['state_dim']), dtype=torch.float32)\r\n        #     # Reset environment and simulate with random actions\r\n        #     x_trial[0] = self.env.reset()\r\n        #     self.action = self.env.get_action(args['max_ep_steps']-1)\r\n        #     for t in range(1, args['max_ep_steps']):\r\n        #         step_info = self.env.step(t, self.action)\r\n        #         x_trial[t] = torch.squeeze(step_info[0])\r\n        #\r\n        #         if step_info[3]['data_collection_done']:\r\n        #             break\r\n        #\r\n        #     done_list.append(step_info[3]['data_collection_done'])\r\n        #     length_list.append(t)\r\n        #     j = 0\r\n        #     while j + self.seq_length < len(x_trial):\r\n        #         x.append(x_trial[j:j + self.seq_length])\r\n        #         # u.append(self.u_trial[j:j + self.seq_length - 1])\r\n        #         u.append(self.action[j:j + self.seq_length-1])\r\n        #         j+=1\r\n        #\r\n        #     if len(x) >= args['total_data_size']:\r\n        #         break\r\n        #     bar.update(len(x))\r\n        # bar.finish()\r\n\r\n        # Continuous open-loop generation\r\n        x_trial = torch.zeros((args['total_data_size'] * args['pred_horizon'], args['state_dim']), dtype=torch.float32)\r\n        x_trial[0] = self.env.reset()\r\n        self.action = self.env.get_action(args['total_data_size'] * args['pred_horizon'] - 1)\r\n        bar = progressbar.ProgressBar(max_value=args['total_data_size'] * args['pred_horizon']).start()\r\n        for t in bar(range(1, args['total_data_size'] * args['pred_horizon'])):\r\n            step_info = self.env.step(t, self.action)\r\n            x_trial[t] = torch.squeeze(step_info[0])\r\n        j = 0\r\n        while j + self.seq_length < len(x_trial):\r\n            x.append(x_trial[j:j + self.seq_length])\r\n            u.append(self.action[j:j + self.seq_length - 1])\r\n            j += 1 + self.seq_length\r\n            bar.update(j)  # Update progress bar\r\n\r\n        # Generate test scenario\r\n        self.x_test = []\r\n        self.x_test.append(self.env.reset())\r\n        action = self.env.get_action(args['max_ep_steps']*args['test_steps'])\r\n        self.u_test = action.to(args['device'])\r\n        for t in range(1, args['max_ep_steps']*args['test_steps']):\r\n            step_info = self.env.step(t, self.u_test)\r\n            # self.u_test.append(step_info[1])\r\n            self.x_test.append(np.squeeze(step_info[0]))\r\n            if step_info[3]['data_collection_done']:\r\n                break\r\n\r\n        x = torch.stack(x).to(args['device'])\r\n        u = torch.stack(u).to(args['device']).float()\r\n        self.x = x.reshape(-1, self.seq_length, args['state_dim']).to(args['device'])\r\n        self.u = u.reshape(-1, self.seq_length-1, args['act_dim']).to(args['device'])\r\n        len_x = int(np.floor(len(self.x)/args['batch_size'])*args['batch_size'])\r\n        self.x = self.x[:len_x]\r\n        self.u = self.u[:len_x]\r\n\r\n        self.dataset_train = MyDataSet(test = False, x = self.x, y = self.u)\r\n        self.shift_ = self.dataset_train.determine_shift_and_scale(args)\r\n        self.dataset_train.shift_scale()\r\n\r\n        x = torch.stack(self.x_test).to(args['device'])\r\n        u = self.u_test[:-1, :]\r\n\r\n        # Reshape and trim data sets\r\n        self.x_test = x.reshape(-1, x.shape[0], args['state_dim']).to(args['device'])\r\n        # self.u_test = u.reshape(-1, x.shape[0]-1, args['act_dim'])\r\n        self.u_test = u.reshape(-1, x.shape[0]-1, args['act_dim']).to(args['device'])\r\n\r\n        self.dataset_test = MyDataSet(test = True, x = self.x_test, y = self.u_test)\r\n        self.dataset_test.shift_scale(self.shift_)\r\n\r\n        len_train = len(self.dataset_train)\r\n        len_val = int(np.round(len_train*args['val_frac']))\r\n        len_train -= len_val\r\n        self.train_subset, self.val_subset = random_split(self.dataset_train,[len_train, len_val],generator=torch.Generator().manual_seed(1))\r\n\r\n\r\n        torch.save(self.dataset_test,args['SAVE_TEST'])\r\n        torch.save(self.x_test,args['SAVE_TEST_X'])\r\n        torch.save(self.u_test, args['SAVE_TEST_U'])\r\n        torch.save(self.train_subset,args['SAVE_TRAIN'])\r\n        torch.save(self.val_subset,args['SAVE_VAL'])\r\n\r\n        print(\"save_test_train_dataset！\")\r\n\r\n    def _store_test(self):\r\n        pass\r\n\r\n    def update_data(self, x_new, u_new, val_frac):\r\n        \"\"\"Update training/validation data\r\n        TODO:\r\n        Args:\r\n            x_new: New state values\r\n            u_new: New control inputs\r\n            val_frac: Fraction of new data to include in validation set\r\n        \"\"\"\r\n        pass\r\n\r\n    def save_data(self, path):\r\n        os.makedirs(path, exist_ok=True)\r\n        torch.save(path + '/x.pt', self.x)\r\n        torch.save(path + '/u.pt', self.u)\r\n        torch.save(path + '/x_test.pt', self.x_test)\r\n        torch.save(path + '/u_test.pt', self.u_test)\r\n        torch.save(path + '/x_val.pt', self.x_val)\r\n        torch.save(path + '/u_val.pt', self.u_val)\r\n\r\n    def _restore_data(self, path):\r\n        self.x = torch.load(path + '/x.pt')\r\n        self.u = torch.load(path + '/u.pt')\r\n        self.x_val = torch.load(path + '/x_val.pt')\r\n        self.u_val = torch.load(path + '/u_val.pt')\r\n        self.x_test = torch.load(path + '/x_test.pt')\r\n        self.u_test = torch.load(path + '/u_test.pt')\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/replay_memory.py b/replay_memory.py
--- a/replay_memory.py	(revision 7ab1852fc127f85716683578724a4afd6f5e8c57)
+++ b/replay_memory.py	(date 1719540667580)
@@ -92,53 +92,43 @@
         x = []
         u = []
 
-        # every trajectory reset the environment
-        # # Define progress bar
-        # bar = progressbar.ProgressBar(maxval=args['total_data_size']).start()
-        # length_list = []
-        # done_list = []
-        # # Loop through episodes
-        # while True:
-        #     # Define arrays to hold observed states and actions in each trial
-        #     x_trial = torch.zeros((args['max_ep_steps'], args['state_dim']), dtype=torch.float32)
-        #     # Reset environment and simulate with random actions
-        #     x_trial[0] = self.env.reset()
-        #     self.action = self.env.get_action(args['max_ep_steps']-1)
-        #     for t in range(1, args['max_ep_steps']):
-        #         step_info = self.env.step(t, self.action)
-        #         x_trial[t] = torch.squeeze(step_info[0])
-        #
-        #         if step_info[3]['data_collection_done']:
-        #             break
-        #
-        #     done_list.append(step_info[3]['data_collection_done'])
-        #     length_list.append(t)
-        #     j = 0
-        #     while j + self.seq_length < len(x_trial):
-        #         x.append(x_trial[j:j + self.seq_length])
-        #         # u.append(self.u_trial[j:j + self.seq_length - 1])
-        #         u.append(self.action[j:j + self.seq_length-1])
-        #         j+=1
-        #
-        #     if len(x) >= args['total_data_size']:
-        #         break
-        #     bar.update(len(x))
-        # bar.finish()
-
-        # Continuous open-loop generation
-        x_trial = torch.zeros((args['total_data_size'] * args['pred_horizon'], args['state_dim']), dtype=torch.float32)
-        x_trial[0] = self.env.reset()
-        self.action = self.env.get_action(args['total_data_size'] * args['pred_horizon'] - 1)
-        bar = progressbar.ProgressBar(max_value=args['total_data_size'] * args['pred_horizon']).start()
-        for t in bar(range(1, args['total_data_size'] * args['pred_horizon'])):
-            step_info = self.env.step(t, self.action)
-            x_trial[t] = torch.squeeze(step_info[0])
-        j = 0
-        while j + self.seq_length < len(x_trial):
-            x.append(x_trial[j:j + self.seq_length])
-            u.append(self.action[j:j + self.seq_length - 1])
-            j += 1 + self.seq_length
-            bar.update(j)  # Update progress bar
+        # Define progress bar
+        bar = progressbar.ProgressBar(maxval=args['total_data_size']).start()
+        length_list = []
+        done_list = []
+        # Loop through episodes
+        while True:
+            # Define arrays to hold observed states and actions in each trial
+            x_trial = torch.zeros((args['max_ep_steps'], args['state_dim']), dtype=torch.float32)
+            # u_trial = torch.zeros((args['max_ep_steps']-1, args['act_dim']), dtype=torch.float32)
+            # Reset environment and simulate with random actions
+            x_trial[0] = self.env.reset()
+            self.action = self.env.get_action(args['max_ep_steps'] - 1)
+            for t in range(1, args['max_ep_steps']):
+                step_info = self.env.step(t, self.action)
+                # print(u_trial.shape)
+                # print(step_info[1].shape)
+                # print(np.squeeze(step_info[0]).shape)
+                # u_trial[t - 1] = step_info[1]
+                x_trial[t] = torch.squeeze(step_info[0])
+
+                if step_info[3]['data_collection_done']:
+                    break
+
+            done_list.append(step_info[3]['data_collection_done'])
+            length_list.append(t)
+            j = 0
+            while j + self.seq_length < len(x_trial):
+                x.append(x_trial[j:j + self.seq_length])
+                # u.append(self.u_trial[j:j + self.seq_length - 1])
+                u.append(self.action[j:j + self.seq_length - 1])
+                j += 1
+
+            if len(x) >= args['total_data_size']:
+                break
+            bar.update(len(x))
+        bar.finish()
+
 
         # Generate test scenario
         self.x_test = []
@@ -147,7 +137,6 @@
         self.u_test = action.to(args['device'])
         for t in range(1, args['max_ep_steps']*args['test_steps']):
             step_info = self.env.step(t, self.u_test)
-            # self.u_test.append(step_info[1])
             self.x_test.append(np.squeeze(step_info[0]))
             if step_info[3]['data_collection_done']:
                 break
